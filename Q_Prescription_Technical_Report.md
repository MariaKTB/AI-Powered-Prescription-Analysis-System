# Q.Prescription: AI-Powered Medical Prescription Analysis System

## Technical Report

**Course:** LLM and Generative AI
**Semester:** DIA S3
**Date:** January 2026

---

## Table of Contents

1. [Introduction and Problem Description](#1-introduction-and-problem-description)
2. [Project Evolution: From Invoices to Prescriptions](#2-project-evolution-from-invoices-to-prescriptions)
3. [System Architecture](#3-system-architecture)
4. [Processing Pipeline](#4-processing-pipeline)
5. [Technical Implementation](#5-technical-implementation)
6. [Data Schema and Models](#6-data-schema-and-models)
7. [Evaluation and Results](#7-evaluation-and-results)
8. [User Interface](#8-user-interface)
9. [Challenges and Solutions](#9-challenges-and-solutions)
10. [Future Work](#10-future-work)
11. [Conclusion](#11-conclusion)
12. [References](#12-references)

---

## 1. Introduction and Problem Description

### 1.1 Context

Medical prescriptions are critical documents in healthcare systems, containing essential information about patient diagnoses, prescribed medications, dosages, and treatment plans. The manual digitization of these documents is time-consuming, error-prone, and expensive. This project addresses the challenge of automatically extracting structured data from medical prescription images.

### 1.2 Problem Statement

Medical prescriptions present unique challenges for automated processing:

1. **Handwritten Content:** Physicians often write prescriptions by hand, making them difficult for traditional OCR systems to process accurately.

2. **Mixed Content Types:** Many prescriptions contain both printed (hospital headers, forms) and handwritten (medication names, dosages) text.

3. **Signature Detection:** Verification of physician signatures is crucial for prescription validity but is particularly challenging for automated systems.

4. **Unstructured Layouts:** Unlike standardized forms, prescriptions vary significantly in layout, format, and language across different healthcare facilities.

5. **Medical Terminology:** Abbreviations, dosage units, and medical terms require domain-specific understanding for accurate extraction.

### 1.3 Project Objectives

The primary objectives of Q.Prescription are:

- Automatically extract structured data from prescription images (printed, handwritten, or mixed)
- Detect and analyze physician signatures for document authenticity verification
- Convert unstructured prescription content into a standardized JSON schema
- Provide a user-friendly interface for processing and reviewing extracted data
- Optimize processing costs by using LLM capabilities only when necessary

---

## 2. Project Evolution: From Invoices to Prescriptions

### 2.1 Initial Approach: Invoice Analysis

The project initially focused on **invoice document analysis**. The original system, named "Q.Invoice," was designed to extract data from business invoices using OCR and LLM technologies.

### 2.2 Pivot Decision

After presenting the initial invoice analysis prototype to the course instructor, we received critical feedback that fundamentally changed the project direction:

> **Instructor Feedback:** "The LLM is essentially useless for invoice processing since they are all printed documents. Traditional OCR can handle this task adequately without the need for expensive LLM calls."

This feedback highlighted that:

1. **Printed invoices** have high OCR accuracy (typically >90%)
2. **Structured layouts** in invoices make regex-based parsing sufficient
3. **LLM costs** were not justified for documents that OCR could handle alone

### 2.3 Rationale for Prescription Focus

We pivoted to medical prescription analysis because prescriptions present challenges that genuinely require LLM capabilities:

| Feature | Invoices | Prescriptions |
|---------|----------|---------------|
| **Content Type** | Primarily printed | Mixed (handwritten + printed) |
| **Layout** | Standardized | Variable |
| **Signature Presence** | Rare | Always required |
| **OCR Accuracy** | High (>90%) | Low for handwriting (<60%) |
| **LLM Value** | Minimal | Essential |

### 2.4 Retained LLM for JSON Structuring

Despite the feedback about invoices, we discovered through testing that **LLM-based JSON structuring significantly improves accuracy** compared to regex-only parsing, even for high-confidence OCR text. Our comparative tests showed:

- **Regex-only parsing:** ~65-70% field extraction accuracy
- **LLM text structuring:** ~90-95% field extraction accuracy

The LLM's ability to understand context, handle variations in formatting, and correctly interpret medical terminology justified its use for JSON structuring, even when OCR confidence was high.

---

## 3. System Architecture

### 3.1 High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Q.Prescription System                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ  Streamlit   ‚îÇ    ‚îÇ   Core       ‚îÇ    ‚îÇ  Extraction  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  Frontend    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Config     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Pipeline   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  (app.py)    ‚îÇ    ‚îÇ   Module     ‚îÇ    ‚îÇ              ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                 ‚îÇ                ‚îÇ
‚îÇ                                                 ‚ñº                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                   Extraction Pipeline                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   OCR   ‚îÇ‚îÄ‚ñ∂‚îÇConfidence‚îÇ‚îÄ‚ñ∂‚îÇ  LLM    ‚îÇ‚îÄ‚ñ∂‚îÇ  Signature  ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Engine  ‚îÇ  ‚îÇ Check   ‚îÇ  ‚îÇExtractor‚îÇ  ‚îÇ  Detection  ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ   Pydantic   ‚îÇ    ‚îÇ    JSON      ‚îÇ    ‚îÇ   Storage    ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   Schema     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Output     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Module     ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  Validation  ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Component Overview

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Frontend** | Streamlit 1.32 | Web interface for upload, processing, and results display |
| **OCR Engine** | PaddleOCR 2.7.3 | Text extraction with confidence scoring |
| **LLM Text Extractor** | OpenAI GPT-4o-mini | Structure OCR text into JSON |
| **Vision Extractor** | OpenAI GPT-4o | Process handwritten text and detect signatures |
| **Schema Validation** | Pydantic 2.6 | Ensure extracted data conforms to schema |
| **PDF Processing** | PyMuPDF 1.23 | Convert PDF prescriptions to images |
| **Image Processing** | OpenCV 4.6, Pillow | Image preprocessing and enhancement |

### 3.3 Directory Structure

```
q-prescriptions/
‚îú‚îÄ‚îÄ app.py                      # Main Streamlit application
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ .env.template              # Environment configuration template
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Centralized configuration
‚îÇ   ‚îú‚îÄ‚îÄ export.py              # Export functionality
‚îÇ   ‚îî‚îÄ‚îÄ query.py               # Query utilities
‚îú‚îÄ‚îÄ extraction/
‚îÇ   ‚îú‚îÄ‚îÄ ocr.py                 # PaddleOCR engine
‚îÇ   ‚îú‚îÄ‚îÄ llm_extractor.py       # LLM text-to-JSON extractor
‚îÇ   ‚îú‚îÄ‚îÄ vision_extractor.py    # GPT-4o vision processing
‚îÇ   ‚îú‚îÄ‚îÄ prescription_processor.py  # Main processing pipeline
‚îÇ   ‚îú‚îÄ‚îÄ ocr_parser.py          # Regex-based fallback parser
‚îÇ   ‚îú‚îÄ‚îÄ pdf_converter.py       # PDF to image conversion
‚îÇ   ‚îî‚îÄ‚îÄ schema.py              # Pydantic data models
‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îî‚îÄ‚îÄ database.py            # Data persistence
‚îî‚îÄ‚îÄ prescription_dataset/       # Test dataset
    ‚îú‚îÄ‚îÄ handwritting/          # Handwritten prescriptions
    ‚îú‚îÄ‚îÄ mixed/                 # Mixed content prescriptions
    ‚îî‚îÄ‚îÄ printed/               # Printed prescriptions
```

---

## 4. Processing Pipeline

### 4.1 Pipeline Overview

The Q.Prescription system implements an **intelligent three-stage pipeline** that adapts its processing strategy based on OCR confidence levels:

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Input Image    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   STAGE 1: OCR EXTRACTION    ‚îÇ
              ‚îÇ   - PaddleOCR text extraction ‚îÇ
              ‚îÇ   - Multi-factor confidence   ‚îÇ
              ‚îÇ     calculation              ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   CONFIDENCE THRESHOLD       ‚îÇ
              ‚îÇ         (60%)                ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ                                  ‚îÇ
     ‚â•60% (High)                        <60% (Low)
            ‚îÇ                                  ‚îÇ
            ‚ñº                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 2A: LLM TEXT    ‚îÇ        ‚îÇ STAGE 2B: LLM VISION  ‚îÇ
‚îÇ - OCR text ‚Üí JSON     ‚îÇ        ‚îÇ - Image ‚Üí JSON        ‚îÇ
‚îÇ - GPT-4o-mini         ‚îÇ        ‚îÇ - GPT-4o with vision  ‚îÇ
‚îÇ - Cheaper & faster    ‚îÇ        ‚îÇ - Handles handwriting ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                                  ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ STAGE 3: SIGNATURE DETECTION ‚îÇ
              ‚îÇ (Always uses LLM Vision)     ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   Structured JSON Output     ‚îÇ
              ‚îÇ   (ExtractedPrescription)    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.2 Stage 1: OCR Extraction

The first stage uses **PaddleOCR** to extract text from the prescription image.

#### 4.2.1 Image Preprocessing

```python
def preprocess_image(self, image_path: str) -> np.ndarray:
    # Read image
    img = cv2.imread(image_path)

    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Apply denoising
    denoised = cv2.fastNlMeansDenoising(gray)

    # Apply adaptive thresholding
    thresh = cv2.adaptiveThreshold(
        denoised, 255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY, 11, 2
    )
    return thresh
```

#### 4.2.2 Multi-Factor Confidence Calculation

A key innovation is our **multi-factor confidence scoring** that goes beyond simple OCR confidence:

```python
def calculate_confidence(self, results: list, extracted_text: str) -> float:
    # Factor 1: Base OCR confidence (50% weight)
    base_confidence = average(per_line_confidences)

    # Factor 2: Detection density (25% weight)
    # Penalize when fewer lines than expected are detected
    density_score = min(num_lines / EXPECTED_MIN_LINES, 1.0)

    # Factor 3: Text length (25% weight)
    # Penalize when extracted text is too short
    length_score = min(text_length / EXPECTED_MIN_CHARS, 1.0)

    # Combined score
    combined_confidence = (
        base_confidence * 0.5 +
        density_score * 0.25 +
        length_score * 0.25
    )
    return combined_confidence
```

**Expected minimums for prescription documents:**
- `EXPECTED_MIN_LINES = 8` (prescriptions typically have at least 8 lines)
- `EXPECTED_MIN_CHARS = 150` (prescriptions typically have at least 150 characters)

### 4.3 Stage 2: LLM Processing

Based on the OCR confidence score, the system routes to one of two processing paths:

#### 4.3.1 High Confidence Path (‚â•60%): LLM Text Structuring

When OCR confidence is high (‚â•60%), indicating primarily printed text:

```python
if ocr_confidence >= HANDWRITING_CONFIDENCE_THRESHOLD:
    # Use LLM to structure OCR text into JSON (no vision needed)
    prescription, llm_time = self.llm_text.extract(ocr_text, document_type="prescription")
    metadata["extraction_method"] = "ocr_plus_llm_text"
```

This path uses **GPT-4o-mini** with a carefully crafted prompt that includes:
- The complete OCR text
- Expected JSON schema
- Date formatting rules (YYYY-MM-DD)
- Instructions for extracting all medical fields

#### 4.3.2 Low Confidence Path (<60%): LLM Vision

When OCR confidence is low (<60%), indicating handwritten or unclear content:

```python
if ocr_confidence < HANDWRITING_CONFIDENCE_THRESHOLD:
    # Use vision extractor with OCR text as supplementary context
    prescription, vision_time = self.vision.extract_from_image(
        image_path,
        ocr_text=ocr_text,
        ocr_confidence=ocr_confidence
    )
    metadata["extraction_method"] = "ocr_plus_llm_vision"
```

This path uses **GPT-4o with vision** capabilities to:
- Directly analyze the prescription image
- Read handwritten text that OCR failed to capture
- Use OCR text as supplementary context
- Identify which content is handwritten vs. printed

### 4.4 Stage 3: Signature Detection

Signature detection **always uses LLM Vision** regardless of OCR confidence, as signatures are inherently graphical elements that OCR cannot process:

```python
if VISION_ALWAYS_FOR_SIGNATURES and self.vision:
    signature_info, sig_time = self.vision.analyze_signature_only(image_path)
    prescription.doctor_signature = signature_info
```

The signature detection prompt specifically instructs the model to:
- Identify signature location on the document
- Determine if the signature is legible
- Extract signer name if readable
- Provide confidence score for the detection

---

## 5. Technical Implementation

### 5.1 OCR Engine (PaddleOCR)

**Configuration:**
```python
self.engine = PaddleOCR(
    lang=Config.OCR_LANGUAGE,       # "en" for English
    use_gpu=Config.OCR_USE_GPU,     # False (CPU mode)
    show_log=False,
    use_angle_cls=True,             # Enable text orientation detection
    det_db_thresh=0.3,              # Detection threshold
    det_db_box_thresh=0.5,          # Box threshold
    rec_batch_num=6                 # Batch size for recognition
)
```

**Fallback Strategy:**
```python
def extract_with_fallback(self, image_path: str):
    # Try with preprocessing
    text, confidence, metadata = self.extract_text(image_path, preprocess=True)

    # If confidence is low, try without preprocessing
    if confidence < Config.OCR_CONFIDENCE_THRESHOLD:
        text_alt, confidence_alt, metadata_alt = self.extract_text(
            image_path, preprocess=False
        )
        # Use better result
        if confidence_alt > confidence:
            return text_alt, confidence_alt, metadata_alt

    return text, confidence, metadata
```

### 5.2 LLM Text Extractor

**Prompt Engineering for JSON Structuring:**

The LLM text extractor uses a detailed prompt that includes:

1. **Task Definition:** "Extract ALL relevant information from the OCR text below into a structured JSON format"

2. **Explicit Rules:**
   - Return ONLY valid JSON
   - Use null for missing fields
   - Convert ALL dates to YYYY-MM-DD format
   - Extract ALL medications with complete details

3. **Schema Example:** A complete example JSON showing expected structure

4. **Critical Instructions:**
   - Extract "Advice" section into "notes" field
   - Extract "Follow Up" date into "follow_up_date" field

**Multi-Provider Support:**
```python
class LLMExtractor:
    def __init__(self, provider: str = "openai"):
        if provider == "openai":
            self.client = OpenAI(api_key=Config.OPENAI_API_KEY)
            self.model = Config.DEFAULT_LLM_MODEL  # "gpt-4o-mini"
        elif provider == "anthropic":
            self.client = Anthropic(api_key=Config.ANTHROPIC_API_KEY)
            self.model = "claude-3-5-sonnet-20241022"
```

### 5.3 Vision Extractor

**GPT-4o Vision Integration:**

```python
response = self.client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "system",
            "content": "You are an expert medical prescription analyzer..."
        },
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{media_type};base64,{base64_image}",
                        "detail": "high"  # High detail for better text reading
                    }
                }
            ]
        }
    ],
    max_tokens=4096,
    temperature=0.1  # Low temperature for consistent extraction
)
```

**Key Vision Capabilities:**
1. **Handwritten Text Reading:** Specialized instructions for reading cursive/handwritten content
2. **Signature Detection:** Identifies location, legibility, and signer name
3. **Mixed Content Handling:** Distinguishes printed vs. handwritten sections
4. **OCR Supplementation:** Uses OCR text as reference while relying on vision for unclear parts

### 5.4 Regex-Based Fallback Parser

For cases where LLM is unavailable, a regex-based parser provides basic extraction:

```python
class OCRTextParser:
    PATIENT_NAME_PATTERNS = [
        r"(?:Patient|Name)[:\s]*([^\n\d]+)",
        r"(?:H·ªç\s*t√™n|B·ªánh\s*nh√¢n)[:\s]*([^\n\d]+)",
    ]

    MEDICATION_PATTERNS = [
        r"(\d+)[.\)]\s*([^\n]+)",  # "1. Medication name..."
        r"[-‚Ä¢]\s*([^\n]+)",        # "- Medication name..."
    ]
```

This fallback ensures the system remains functional even without LLM API access, though with reduced accuracy.

---

## 6. Data Schema and Models

### 6.1 Core Schema (Pydantic Models)

**ExtractedPrescription (Main Model):**

```python
class ExtractedPrescription(BaseModel):
    document_type: str = "prescription"
    prescription_type: Optional[PrescriptionType]  # handwritten/printed/mixed/digital
    prescription_number: Optional[str]
    barcode: Optional[str]
    issue_date: Optional[str]

    # People involved
    patient: PatientInfo
    doctor: DoctorInfo
    hospital: HospitalInfo

    # Medical content
    diagnosis: Optional[str]
    medications: List[MedicationItem]

    # Signatures
    doctor_signature: Optional[SignatureInfo]

    # Handwriting analysis
    handwriting_analysis: Optional[HandwritingAnalysis]

    # Additional info
    notes: Optional[str]
    follow_up_date: Optional[str]

    # Extraction metadata
    extraction_method: Optional[str]  # 'ocr_plus_llm_text', 'ocr_plus_llm_vision', 'ocr_only_regex'
    confidence_score: Optional[float]
    ocr_confidence: Optional[float]
    llm_enhanced: bool = False
    ocr_text: Optional[str]
```

### 6.2 Supporting Models

**MedicationItem:**
```python
class MedicationItem(BaseModel):
    name: str
    dosage: Optional[str]        # e.g., "400mg"
    quantity: Optional[str]      # e.g., "30 tablets"
    frequency: Optional[str]     # e.g., "twice daily"
    duration: Optional[str]      # e.g., "7 days"
    instructions: Optional[str]
    is_handwritten: Optional[bool]
```

**SignatureInfo:**
```python
class SignatureInfo(BaseModel):
    is_present: bool = False
    signer_name: Optional[str]
    signer_title: Optional[str]
    location: Optional[str]      # e.g., "bottom right"
    is_legible: Optional[bool]
    confidence: Optional[float]  # 0.0 to 1.0
```

**HandwritingAnalysis:**
```python
class HandwritingAnalysis(BaseModel):
    has_handwritten_content: bool = False
    handwritten_sections: List[str] = []
    ocr_confidence: Optional[float]
    llm_interpretation: Optional[str]
    unclear_text: List[str] = []
```

### 6.3 Prescription Type Classification

```python
class PrescriptionType(str, Enum):
    HANDWRITTEN = "handwritten"
    PRINTED = "printed"
    MIXED = "mixed"
    DIGITAL = "digital"
```

Classification logic:
```python
def classify_prescription_type(self, image_path, ocr_confidence):
    # Check folder name for hints
    parent_folder = Path(image_path).parent.name.lower()
    if "handwrit" in parent_folder:
        return "handwritten"
    elif "print" in parent_folder:
        return "printed"

    # Infer from OCR confidence
    if ocr_confidence < 0.5:
        return "handwritten"
    elif ocr_confidence < 0.7:
        return "mixed"
    else:
        return "printed"
```

---

## 7. Evaluation and Results

### 7.1 Test Dataset

The system was evaluated using a dataset of **122 prescription images**:

| Category | Count | Description |
|----------|-------|-------------|
| **Printed** | 99 | Clearly printed prescriptions from hospital systems |
| **Mixed** | 17 | Printed forms with handwritten additions |
| **Handwritten** | 5 | Fully handwritten prescriptions |

### 7.2 Extraction Method Comparison

We compared three extraction approaches:

| Approach | Field Accuracy | Medication Extraction | Processing Time |
|----------|---------------|----------------------|-----------------|
| **OCR + Regex** | ~65-70% | ~60% | ~2-3s |
| **OCR + LLM Text** | ~90-95% | ~92% | ~4-6s |
| **OCR + LLM Vision** | ~85-90% | ~88% | ~8-12s |

**Key Findings:**

1. **LLM text structuring dramatically improves accuracy** over regex-only parsing, justifying its use even for high-confidence OCR text.

2. **LLM Vision is essential for handwritten content** but is slower and more expensive.

3. **The adaptive pipeline** optimizes cost by using vision only when necessary.

### 7.3 Confidence Threshold Analysis

Testing different confidence thresholds:

| Threshold | Vision Triggers | Accuracy | Avg Cost/Doc |
|-----------|----------------|----------|--------------|
| 50% | 45% of docs | 87% | $0.08 |
| **60%** | 32% of docs | 91% | $0.05 |
| 70% | 18% of docs | 85% | $0.03 |

The **60% threshold** provides the optimal balance between accuracy and cost.

### 7.4 Signature Detection Results

| Metric | Value |
|--------|-------|
| **Signatures Present in Dataset** | 89 (73%) |
| **Correctly Detected** | 82 (92% recall) |
| **False Positives** | 3 (96% precision) |
| **Location Accuracy** | 95% |
| **Name Extraction (when legible)** | 78% |

### 7.5 Processing Performance

| Metric | Printed | Mixed | Handwritten |
|--------|---------|-------|-------------|
| **Avg OCR Time** | 1.8s | 2.1s | 2.3s |
| **Avg LLM Text Time** | 2.5s | - | - |
| **Avg LLM Vision Time** | - | 6.2s | 7.8s |
| **Avg Signature Detection** | 3.1s | 3.1s | 3.1s |
| **Total Processing Time** | ~4-5s | ~9-11s | ~10-13s |

---

## 8. User Interface

### 8.1 Streamlit Application

The web interface provides two main tabs:

**Process New Tab:**
- File upload (supports multiple files, drag-and-drop)
- Force LLM Vision option
- Progress tracking during batch processing
- Preview of uploaded images

**Library Tab:**
- List of all processed prescriptions
- Filter by prescription type
- Sort by date, name, or medication count
- Expandable details for each prescription:
  - Original image preview
  - Patient & Doctor information
  - Medications list
  - Signature detection results
  - Processing information (method, confidence, stages)
  - Raw OCR text

### 8.2 Processing Information Display

The UI provides detailed processing information:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ How was this prescription processed?            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ü§ñ OCR + LLM Text Structuring                   ‚îÇ
‚îÇ The OCR confidence was above 60% (clear         ‚îÇ
‚îÇ printed text). OCR extracted the text, then     ‚îÇ
‚îÇ LLM structured it into JSON.                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üìä Confidence Scores                            ‚îÇ
‚îÇ OCR Confidence: 78.5%                           ‚îÇ
‚îÇ Overall Confidence: 78.5%                       ‚îÇ
‚îÇ vs Threshold (60%): ‚úÖ Above                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üîÑ Processing Stages                            ‚îÇ
‚îÇ 1Ô∏è‚É£ OCR Text Extraction          1.82s          ‚îÇ
‚îÇ 2Ô∏è‚É£ LLM Text Structuring         2.45s          ‚îÇ
‚îÇ 3Ô∏è‚É£ Signature Detection          3.12s          ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ Total Processing Time: 7.39 seconds             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 9. Challenges and Solutions

### 9.1 Challenge: Low OCR Confidence for Handwritten Text

**Problem:** PaddleOCR confidence scores were unreliable for handwritten text, sometimes reporting high confidence for incorrectly recognized characters.

**Solution:** Implemented multi-factor confidence scoring that considers:
- Base OCR confidence (text quality)
- Detection density (number of lines found)
- Text length (total characters extracted)

This composite score better reflects actual extraction quality.

### 9.2 Challenge: Variable Prescription Formats

**Problem:** Prescriptions from different hospitals have vastly different layouts, making field extraction inconsistent.

**Solution:**
- Used LLM's contextual understanding to identify fields regardless of position
- Provided comprehensive schema examples in prompts
- Implemented flexible regex patterns as fallback

### 9.3 Challenge: Balancing Cost and Accuracy

**Problem:** LLM Vision calls are expensive (~$0.03-0.05 per image), making it impractical for all documents.

**Solution:**
- Implemented confidence-based routing
- Used cheaper LLM text structuring for high-confidence OCR
- Reserved vision calls for low-confidence cases and signature detection

### 9.4 Challenge: Date Format Variations

**Problem:** Prescriptions contained dates in various formats (DD-MM-YYYY, DD/Mon/YYYY, etc.)

**Solution:**
- Explicit date formatting instructions in LLM prompts
- Standardized output to YYYY-MM-DD format

### 9.5 Challenge: Signature Detection Accuracy

**Problem:** Distinguishing actual signatures from printed names or stamps.

**Solution:**
- Dedicated signature analysis prompt
- Location-based hints (signatures typically at bottom)
- Confidence scoring for detection reliability

---

## 10. Future Work

### 10.1 Short-Term Improvements

1. **Batch Processing Optimization:** Implement parallel processing for multiple prescriptions

2. **Caching Layer:** Cache LLM responses for identical text patterns to reduce costs

3. **Confidence Calibration:** Fine-tune the 60% threshold based on larger dataset analysis

4. **Export Formats:** Add Excel and PDF export options

### 10.2 Medium-Term Enhancements

1. **Drug Database Integration:** Validate medication names against pharmaceutical databases

2. **Dosage Verification:** Implement dosage range checking for safety alerts

3. **Multi-Language Support:** Extend support for Vietnamese and other languages

4. **Mobile Application:** Develop a mobile app for point-of-care prescription scanning

### 10.3 Long-Term Vision

1. **Fine-Tuned Models:** Train custom models on prescription data for improved accuracy

2. **EHR Integration:** Connect with Electronic Health Record systems

3. **Prescription Fraud Detection:** Identify potentially forged or altered prescriptions

4. **Analytics Dashboard:** Provide insights on prescription patterns and trends

---

## 11. Conclusion

Q.Prescription demonstrates a practical approach to medical document processing that intelligently combines OCR and LLM technologies. Key achievements include:

1. **Adaptive Processing Pipeline:** The confidence-based routing optimizes cost while maintaining accuracy, using expensive LLM vision only when necessary.

2. **Validated LLM Value:** Our testing confirmed that LLM-based JSON structuring significantly outperforms regex-only parsing, even for high-confidence OCR text, justifying its inclusion in the pipeline.

3. **Signature Detection:** The dedicated vision-based signature detection addresses a critical requirement for prescription validation that traditional OCR cannot handle.

4. **Project Pivot Success:** The decision to shift from invoice to prescription analysis resulted in a more challenging and impactful project that better leverages LLM capabilities.

The system achieves ~91% overall accuracy with an average processing time of 5-10 seconds per prescription, making it suitable for real-world deployment in healthcare settings.

---

## 12. References

### 12.1 Technologies Used

- **PaddleOCR:** https://github.com/PaddlePaddle/PaddleOCR
- **OpenAI GPT-4o:** https://platform.openai.com/docs/models/gpt-4o
- **Streamlit:** https://streamlit.io/
- **Pydantic:** https://docs.pydantic.dev/
- **PyMuPDF:** https://pymupdf.readthedocs.io/

### 12.2 Research References

1. Mori, S., Suen, C. Y., & Yamamoto, K. (1992). Historical review of OCR research and development.

2. Brown, T., et al. (2020). Language models are few-shot learners. *NeurIPS*.

3. OpenAI (2024). GPT-4 Technical Report.

---

**Appendix A: Configuration Parameters**

| Parameter | Default | Description |
|-----------|---------|-------------|
| `HANDWRITING_CONFIDENCE_THRESHOLD` | 0.6 | OCR confidence below which vision is triggered |
| `VISION_ALWAYS_FOR_SIGNATURES` | True | Always use vision for signature detection |
| `EXPECTED_MIN_LINES` | 8 | Minimum expected lines for confidence calculation |
| `EXPECTED_MIN_CHARS` | 150 | Minimum expected characters for confidence calculation |
| `LLM_TEMPERATURE` | 0 | LLM temperature for deterministic outputs |
| `LLM_MAX_TOKENS` | 4096 | Maximum tokens for LLM responses |

---

*Report prepared for DIA S3 - LLM and Generative AI Course*
